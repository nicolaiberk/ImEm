{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05 Training the classifier\n",
    "\n",
    "With the additional data, I should now be able to train a classifier. First I split the data into subsamples for crossvalidation according to the random groups I assigned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os.path\n",
    "basedir = os.path.expanduser('~/Dropbox/Studies/Semester 2/Block I/data_IMEM/')\n",
    "\n",
    "test = [[], [], [], [], []]\n",
    "train =  [[], [], [], [], []]\n",
    "\n",
    "\n",
    "# define crossvalidation samples\n",
    "with open(basedir + 'intermediate/crossval.csv', mode=\"r\", encoding=\"utf-8\") as crossval:\n",
    "        reader = csv.reader(crossval)\n",
    "        next(reader) # skip header\n",
    "        # crossvalidation samples\n",
    "        for row in reader:\n",
    "            for s in range(5):\n",
    "                if int(row[8]) == s:\n",
    "                    test[s].append(row)\n",
    "                else:\n",
    "                    train[s].append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many positive cases do we have in our samples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample:  1\n",
      "0.14285714285714285\n",
      "103\n",
      "721\n",
      "\n",
      "Sample:  2\n",
      "0.14040114613180515\n",
      "98\n",
      "698\n",
      "\n",
      "Sample:  3\n",
      "0.14603616133518776\n",
      "105\n",
      "719\n",
      "\n",
      "Sample:  4\n",
      "0.15256588072122051\n",
      "110\n",
      "721\n",
      "\n",
      "Sample:  5\n",
      "0.15912208504801098\n",
      "116\n",
      "729\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "for ts in train:\n",
    "    print('\\nSample: ', i)\n",
    "    print(sum([int(t[7]) for t in ts])/len([int(t[7]) for t in ts]))\n",
    "    print(sum([int(t[7]) for t in ts]))\n",
    "    print(len([int(t[7]) for t in ts]))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that each subsample has around 100 positive cases, which constitute around 15% of the training set. Remember that each observation is used five times here: four times in the training set and once in the test set.\n",
    "\n",
    "In the next step, I will define the vectorizers for our text (once a simple count vectorizer; once a 'term-frequency-inverse-document-frequency', which weights the word counts by their frequency across all documents) and fit these vectorizers to my data. Taking into account that I have two types of text, two different vectorizer and five crossvalidation samples, I will end up getting 2x2x5 = 20 vectrizers. the testing data is also transformed to be predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vec_count_f = CountVectorizer(max_df=.5, min_df=5)\n",
    "vec_count_r = CountVectorizer(max_df=.5, min_df=5)\n",
    "vec_tfidf_f = TfidfVectorizer(max_df=.5, min_df=5)\n",
    "vec_tfidf_r = TfidfVectorizer(max_df=.5, min_df=5)\n",
    "\n",
    "count_f = [[], [], [], [], []]\n",
    "count_r = [[], [], [], [], []]\n",
    "tfidf_f = [[], [], [], [], []]\n",
    "tfidf_r = [[], [], [], [], []]\n",
    "\n",
    "test_count_f = [[], [], [], [], []]\n",
    "test_count_r = [[], [], [], [], []]\n",
    "test_tfidf_f = [[], [], [], [], []]\n",
    "test_tfidf_r = [[], [], [], [], []]\n",
    "\n",
    "for s in range(5):\n",
    "    count_f[s] = vec_count_f.fit_transform([t[5] for t in train[s]])\n",
    "    count_r[s] = vec_count_r.fit_transform([t[6] for t in train[s]])\n",
    "    tfidf_f[s] = vec_tfidf_f.fit_transform([t[5] for t in train[s]])\n",
    "    tfidf_r[s] = vec_tfidf_r.fit_transform([t[6] for t in train[s]])\n",
    "    test_count_f[s] = vec_count_f.transform([t[5] for t in test[s]])\n",
    "    test_count_r[s] = vec_count_r.transform([t[5] for t in test[s]])\n",
    "    test_tfidf_f[s] = vec_tfidf_f.transform([t[5] for t in test[s]])\n",
    "    test_tfidf_r[s] = vec_tfidf_r.transform([t[5] for t in test[s]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before I fit the model, I should oversample the data in each validation set to make my data balanced:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "# oversample training samples for crossvalidation\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "y = [[], [], [], [], []]\n",
    "X_count_f = [[], [], [], [], []]\n",
    "X_count_r = [[], [], [], [], []]\n",
    "X_tfidf_f = [[], [], [], [], []]\n",
    "X_tfidf_r = [[], [], [], [], []]\n",
    "y_res = [[], [], [], [], []]\n",
    "\n",
    "\n",
    "for s in range(5):\n",
    "    y[s] = [row[7] for row in train[s]]\n",
    "    X_count_f[s], y_res[s] = SMOTE().fit_resample(count_f[s], y[s])\n",
    "    X_count_r[s], y_res[s] = SMOTE().fit_resample(count_r[s], y[s])\n",
    "    X_tfidf_f[s], y_res[s] = SMOTE().fit_resample(tfidf_f[s], y[s])\n",
    "    X_tfidf_r[s], y_res[s] = SMOTE().fit_resample(tfidf_r[s], y[s])\n",
    "\n",
    " \n",
    "    \n",
    "# test:\n",
    "for i in range(5):\n",
    "    s = 0\n",
    "    for v in y_res[i]:\n",
    "        s += int(v)\n",
    "    print(s/len(y_res[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the distribution of classes in my set, one can see that all subsamples are perfectly balanced now.\n",
    "\n",
    "This means it's finally time to fit some models! I employ a logistic regression, a naive Bayes classifier and a support vector machine to classify the data. Each will be applied to two types of text (pos-tag restricted and full text) and two types of vectorizers, resulting in a total of twelve models. Their performance will be assessed by taking average performance across the five cross-validations of each model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samunico\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), Count, full text:\n",
      "\n",
      "Average accuracy: 0.954378952498765\n",
      "Average precision: 0.8831797676008202\n",
      "Average recall: 0.7969419071976616\n",
      "Average f1-score: 0.8335360656115374\n",
      "\n",
      "\n",
      "\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), Count, restricted text:\n",
      "\n",
      "Average accuracy: 0.9468074601325123\n",
      "Average precision: 0.9267813765182187\n",
      "Average recall: 0.7055133357690903\n",
      "Average f1-score: 0.7967266340217161\n",
      "\n",
      "\n",
      "\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), Tfidf, full text:\n",
      "\n",
      "Average accuracy: 0.9611326764910695\n",
      "Average precision: 0.8862665112665112\n",
      "Average recall: 0.8569102423578127\n",
      "Average f1-score: 0.8655377037951555\n",
      "\n",
      "\n",
      "\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), Tfidf, restricted text:\n",
      "\n",
      "Average accuracy: 0.9239763008339035\n",
      "Average precision: 0.946524064171123\n",
      "Average recall: 0.5225952989891609\n",
      "Average f1-score: 0.670984570984571\n",
      "\n",
      "\n",
      "\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), Count, full text:\n",
      "\n",
      "Average accuracy: 0.9467612699911836\n",
      "Average precision: 0.8438928571428571\n",
      "Average recall: 0.789571306783583\n",
      "Average f1-score: 0.8118341617222585\n",
      "\n",
      "\n",
      "\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), Count, restricted text:\n",
      "\n",
      "Average accuracy: 0.9322244491341379\n",
      "Average precision: 0.9174825174825175\n",
      "Average recall: 0.5859127999025697\n",
      "Average f1-score: 0.7077027595060382\n",
      "\n",
      "\n",
      "\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), Tfidf, full text:\n",
      "\n",
      "Average accuracy: 0.9286360238861511\n",
      "Average precision: 0.682077732813027\n",
      "Average recall: 0.9636853002070394\n",
      "Average f1-score: 0.7936968122627672\n",
      "\n",
      "\n",
      "\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), Tfidf, restricted text:\n",
      "\n",
      "Average accuracy: 0.9312565398860639\n",
      "Average precision: 0.6981098003629764\n",
      "Average recall: 0.9436853002070393\n",
      "Average f1-score: 0.7980492106887732\n",
      "\n",
      "\n",
      "\n",
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), Count, full text:\n",
      "\n",
      "Average accuracy: 0.9489725794691635\n",
      "Average precision: 0.9060610451076982\n",
      "Average recall: 0.7311192303008159\n",
      "Average f1-score: 0.8056372549019608\n",
      "\n",
      "\n",
      "\n",
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), Count, restricted text:\n",
      "\n",
      "Average accuracy: 0.9013615184243606\n",
      "Average precision: 0.9386363636363637\n",
      "Average recall: 0.3634185848252344\n",
      "Average f1-score: 0.513968253968254\n",
      "\n",
      "\n",
      "\n",
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), Tfidf, full text:\n",
      "\n",
      "Average accuracy: 0.9388887962112229\n",
      "Average precision: 0.872027290448343\n",
      "Average recall: 0.701439532334673\n",
      "Average f1-score: 0.7684662349237307\n",
      "\n",
      "\n",
      "\n",
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False), Tfidf, restricted text:\n",
      "\n",
      "Average accuracy: 0.902997699842175\n",
      "Average precision: 0.9666666666666668\n",
      "Average recall: 0.3625185726464498\n",
      "Average f1-score: 0.5253726287262872\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import svm\n",
    "import sklearn.metrics\n",
    "import numpy as np\n",
    "    \n",
    "\n",
    "vecs = ['Count, full text', \n",
    "        'Count, restricted text', \n",
    "        'Tfidf, full text', \n",
    "        'Tfidf, restricted text']\n",
    "trains = [X_count_f, X_count_r, X_tfidf_f, X_tfidf_r]\n",
    "tests = [test_count_f, test_count_r, test_tfidf_f, test_tfidf_r]\n",
    "logreg = LogisticRegression()\n",
    "naive = MultinomialNB()\n",
    "svm_m = svm.SVC(gamma='scale')\n",
    "models = [logreg, naive, svm_m]\n",
    "\n",
    "\n",
    "for m in models:\n",
    "    for vec in range(4):\n",
    "        acc    = []\n",
    "        precis = []\n",
    "        recall = []\n",
    "        f1     = []\n",
    "        for i in range(5):\n",
    "            m.fit(trains[vec][i], y_res[i])\n",
    "            prediction = m.predict(tests[vec][i])\n",
    "            acc.append(sklearn.metrics.accuracy_score([t[7] for t in test[i]], prediction))\n",
    "            precis.append(sklearn.metrics.precision_score([t[7] for t in test[i]], prediction, pos_label='1'))\n",
    "            recall.append(sklearn.metrics.recall_score([t[7] for t in test[i]], prediction, pos_label='1'))\n",
    "            f1.append(sklearn.metrics.f1_score([t[7] for t in test[i]], prediction, pos_label='1'))\n",
    "        print(str(m) + ', ' + str(vecs[vec]) + ':\\n' +\n",
    "              '\\nAverage accuracy: ' + str(np.mean(acc)) +\n",
    "              '\\nAverage precision: ' + str(np.mean(precis)) +\n",
    "              '\\nAverage recall: ' + str(np.mean(recall)) +\n",
    "              '\\nAverage f1-score: ' + str(np.mean(f1)) + '\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that most of them perform very good, with only the support vector machine underperforming on recall when using the tf-idf vectorizer. I will employ the logistic regression with tf-idf vectorizer based on the full text returns the best results, since it performs best.\n",
    "\n",
    "As it is virtually impossible to assign the average coefficients of five cross-validated models to an existing trained r untrained model, I will train a new model based on the full data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samunico\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load full coded training set\n",
    "train_f = []\n",
    "train_y = []\n",
    "with open(basedir + 'intermediate/train.csv', mode=\"r\", encoding=\"utf-8\") as train_d:\n",
    "    reader = csv.reader(train_d)\n",
    "    next(reader) # skip header\n",
    "    for row in reader:\n",
    "        train_f.append(row[5])\n",
    "        train_y.append(row[7])\n",
    "        \n",
    "# fit model\n",
    "tfidf_f = vec_tfidf_f.fit_transform([t for t in train_f])\n",
    "X_tfidf_f, train_y_res = SMOTE().fit_resample(tfidf_f, train_y)\n",
    "logreg.fit(X_tfidf_f, train_y_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this model, we can now predict the test set and assess its performance (I can't use GridSearchCV() since it would result in data bleed to the test set and hence overfitting, see https://medium.com/lumiata/cross-validation-for-imbalanced-datasets-9d203ba47e8 ):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.96\n",
      "Precision: 0.9\n",
      "Recall: 0.85\n",
      "F1-score: 0.87\n",
      "\n",
      "\n",
      "\n",
      "[[496   9]\n",
      " [ 14  78]]\n"
     ]
    }
   ],
   "source": [
    "# load test set\n",
    "from sklearn import metrics\n",
    "test_f = []\n",
    "test_y = []\n",
    "with open(basedir + 'intermediate/test.csv', mode=\"r\", encoding=\"utf-8\") as test_d:\n",
    "    reader = csv.reader(test_d)\n",
    "    next(reader) # skip header\n",
    "    for row in reader:\n",
    "        test_f.append(row[5])\n",
    "        test_y.append(row[7])\n",
    "\n",
    "# transform and predict\n",
    "test_vec = vec_tfidf_f.transform([t for t in test_f])\n",
    "test_pred = logreg.predict(test_vec)\n",
    "\n",
    "# performance?\n",
    "print('Accuracy: ' + str(round(sklearn.metrics.accuracy_score(test_y, test_pred), 2)) + \n",
    "      '\\nPrecision: ' + str(round(sklearn.metrics.precision_score(test_y, test_pred, pos_label='1'), 2)) +\n",
    "      '\\nRecall: ' + str(round(sklearn.metrics.recall_score(test_y, test_pred, pos_label='1'), 2)) +\n",
    "      '\\nF1-score: ' + str(round(sklearn.metrics.f1_score(test_y, test_pred, pos_label='1'), 2)) + '\\n\\n\\n')\n",
    "\n",
    "print(metrics.confusion_matrix(test_y, test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am obviously very happy now because my model performs so well, even without cross-validation. However, it might still be improved by adjusting the threshold. Let's check the ROC-curve and find the optimal threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC: 0.99\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.35"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# ROC-curve:\n",
    "predprobs = logreg.predict_log_proba(test_vec)\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(test_y, predprobs[:,1], pos_label='1')\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "print(\"ROC AUC: {}\".format(round(roc_auc, 3)))\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr,tpr,'b',label='AUC = %0.2f'%roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "plt.xlim([0,1])\n",
    "plt.ylim([0,1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()\n",
    "\n",
    "# ideal cutoff (Youden's J):\n",
    "optimal_idx = np.argmax(tpr-fpr)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "print(round(optimal_threshold, 2))\n",
    "\n",
    "round(np.exp(-1.0461102494062424), 2) # ~0.35 -> we might be able to substantially lower the threshold!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the area under the curve as well as the ROC-curve look very nice. Looking at the ROC-curve, we can see that the optimal threshold should have a true positive rate of a little above .95. Youden's J gives us the optimal (logged) cutoff and - transforming it, we see that we could substantially lower the threshold to 0.35!\n",
    "\n",
    "Let's train this new model and compare performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New model with threshold at ~ 0.35 vs. initial classifier:\n",
      "\n",
      "Accuracy: 0.95 vs. 0.96\n",
      "Precision: 0.75 vs. 0.9\n",
      "Recall: 0.97 vs. 0.85\n",
      "F1-score: 0.85 vs. 0.87\n",
      "[[476  29]\n",
      " [  3  89]]\n"
     ]
    }
   ],
   "source": [
    "# new model:\n",
    "predictions_new = np.where(predprobs[:,1] > optimal_threshold, '1', '0')\n",
    "print('New model with threshold at ~ 0.35 vs. initial classifier:')\n",
    "print('\\nAccuracy: ' + \n",
    "      str(round(sklearn.metrics.accuracy_score(test_y, predictions_new), 2)) + \n",
    "      ' vs. ' + \n",
    "      str(round(sklearn.metrics.accuracy_score(test_y, test_pred), 2)) + \n",
    "      '\\nPrecision: ' + \n",
    "      str(round(sklearn.metrics.precision_score(test_y, predictions_new, pos_label = '1'), 2)) + \n",
    "      ' vs. ' + \n",
    "      str(round(sklearn.metrics.precision_score(test_y, test_pred, pos_label = '1'), 2)) + \n",
    "      '\\nRecall: ' + \n",
    "      str(round(sklearn.metrics.recall_score(test_y, predictions_new, pos_label = '1'), 2)) + \n",
    "      ' vs. ' + \n",
    "      str(round(sklearn.metrics.recall_score(test_y, test_pred, pos_label = '1'), 2)) + \n",
    "      '\\nF1-score: ' + \n",
    "      str(round(sklearn.metrics.f1_score(test_y, predictions_new, pos_label = '1'), 2)) + \n",
    "      ' vs. ' + \n",
    "      str(round(sklearn.metrics.f1_score(test_y, test_pred, pos_label = '1'), 2)))\n",
    "\n",
    "print(metrics.confusion_matrix(test_y, predictions_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we could manage to improve recall substantially, however at the price of precision (accuracy is slightly worse, f1 identical). The trade-off here is whether to be able to collect 10% more (nearly all) of the press releases on immigration at the price of 10% more wrongly classified releases that are not on immigration. I decide to go for very strong performance on both measures over exceptional recall and average precision and stick with the threshold of 0.5.\n",
    "\n",
    "I am thrilled by the performance of my classifier, however - since we did not employ cross-validation for the training of this model - it might be overfitted. To check that the classifier is not guided by spurious relationships, we can visually inspect the best predictor words and check the classification of a press release using eli5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    table.eli5-weights tr:hover {\n",
       "        filter: brightness(85%);\n",
       "    }\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "        \n",
       "\n",
       "    \n",
       "\n",
       "        \n",
       "            \n",
       "                \n",
       "                \n",
       "    \n",
       "        <p style=\"margin-bottom: 0.5em; margin-top: 0em\">\n",
       "            <b>\n",
       "    \n",
       "        y=1\n",
       "    \n",
       "</b>\n",
       "\n",
       "top features\n",
       "        </p>\n",
       "    \n",
       "    <table class=\"eli5-weights\"\n",
       "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; margin-bottom: 2em;\">\n",
       "        <thead>\n",
       "        <tr style=\"border: none;\">\n",
       "            \n",
       "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature weights. Note that weights do not account for feature value scales, so if feature values have different scales, features with highest weights might not be the most important.\">\n",
       "                    Weight<sup>?</sup>\n",
       "                </th>\n",
       "            \n",
       "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "            \n",
       "        </tr>\n",
       "        </thead>\n",
       "        <tbody>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +7.009\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        fluchtling\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 82.67%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +5.710\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        integration\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 85.77%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +4.309\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        ausland\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 88.81%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +3.058\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        zuwander\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 90.93%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +2.266\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        migrant\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 91.60%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +2.031\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        kommun\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 91.65%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +2.012\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        deutschland\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 91.81%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +1.956\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        eu\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 92.12%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +1.853\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        zuwanderungsgesetz\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 92.30%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +1.792\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        zuwand\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 92.60%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +1.692\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        wer\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 92.69%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +1.665\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        illegal\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 93.05%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +1.550\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        staatsangehor\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 93.19%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +1.504\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        spataussiedl\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 93.20%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +1.501\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        uns\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 93.34%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +1.457\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        land\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 93.59%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +1.379\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        migration\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 93.61%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +1.372\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        sprach\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 93.86%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +1.299\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        arbeitsmarkt\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 93.86%); border: none;\">\n",
       "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
       "                    <i>&hellip; 1486 more positive &hellip;</i>\n",
       "                </td>\n",
       "            </tr>\n",
       "        \n",
       "\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 91.96%); border: none;\">\n",
       "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
       "                    <i>&hellip; 2343 more negative &hellip;</i>\n",
       "                </td>\n",
       "            </tr>\n",
       "        \n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 91.96%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -1.906\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        &lt;BIAS&gt;\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "\n",
       "        </tbody>\n",
       "    </table>\n",
       "\n",
       "            \n",
       "        \n",
       "\n",
       "        \n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best predictor words:\n",
    "import eli5\n",
    "eli5.show_weights(logreg, top = 20, vec = vec_tfidf_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    table.eli5-weights tr:hover {\n",
       "        filter: brightness(85%);\n",
       "    }\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "        \n",
       "\n",
       "    \n",
       "\n",
       "        \n",
       "\n",
       "        \n",
       "    \n",
       "        \n",
       "        \n",
       "    \n",
       "        <p style=\"margin-bottom: 0.5em; margin-top: 0em\">\n",
       "            <b>\n",
       "    \n",
       "        y=1\n",
       "    \n",
       "</b>\n",
       "\n",
       "    \n",
       "    (probability <b>0.962</b>, score <b>3.236</b>)\n",
       "\n",
       "top features\n",
       "        </p>\n",
       "    \n",
       "    <table class=\"eli5-weights\"\n",
       "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; margin-bottom: 2em;\">\n",
       "        <thead>\n",
       "        <tr style=\"border: none;\">\n",
       "            \n",
       "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature contribution already accounts for the feature value (for linear models, contribution = weight * feature value), and the sum of feature contributions is equal to the score or, for some classifiers, to the probability. Feature values are shown if &quot;show_feature_values&quot; is True.\">\n",
       "                    Contribution<sup>?</sup>\n",
       "                </th>\n",
       "            \n",
       "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "            \n",
       "        </tr>\n",
       "        </thead>\n",
       "        <tbody>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +5.142\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        Highlighted in text (sum)\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "        \n",
       "\n",
       "        \n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 90.02%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -1.906\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        &lt;BIAS&gt;\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "\n",
       "        </tbody>\n",
       "    </table>\n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n",
       "    <p style=\"margin-bottom: 2.5em; margin-top:-0.5em;\">\n",
       "        <span style=\"background-color: hsl(120, 100.00%, 99.05%); opacity: 0.80\" title=\"0.002\">18</span><span style=\"opacity: 0.80\">.</span><span style=\"background-color: hsl(120, 100.00%, 95.61%); opacity: 0.81\" title=\"0.014\">09</span><span style=\"opacity: 0.80\">.</span><span style=\"background-color: hsl(120, 100.00%, 98.66%); opacity: 0.80\" title=\"0.003\">2015</span><span style=\"opacity: 0.80\">  </span><span style=\"background-color: hsl(0, 100.00%, 94.17%); opacity: 0.81\" title=\"-0.021\">15</span><span style=\"opacity: 0.80\">:</span><span style=\"background-color: hsl(0, 100.00%, 97.99%); opacity: 0.80\" title=\"-0.005\">10</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 94.86%); opacity: 0.81\" title=\"0.018\">gesetzentwurf</span><span style=\"opacity: 0.80\"> der </span><span style=\"background-color: hsl(0, 100.00%, 95.77%); opacity: 0.81\" title=\"-0.013\">bundesregier</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 97.65%); opacity: 0.80\" title=\"0.006\">weist</span><span style=\"opacity: 0.80\"> in die </span><span style=\"background-color: hsl(120, 100.00%, 93.30%); opacity: 0.82\" title=\"0.026\">richtig</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 90.70%); opacity: 0.82\" title=\"0.041\">richtung</span><span style=\"opacity: 0.80\"> zur </span><span style=\"background-color: hsl(120, 100.00%, 89.22%); opacity: 0.83\" title=\"0.051\">bewalt</span><span style=\"opacity: 0.80\"> der </span><span style=\"background-color: hsl(0, 100.00%, 98.07%); opacity: 0.80\" title=\"-0.004\">stark</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 99.98%); opacity: 0.80\" title=\"0.000\">steigend</span><span style=\"opacity: 0.80\"> asylbewerberzahl </span><span style=\"background-color: hsl(120, 100.00%, 98.97%); opacity: 0.80\" title=\"0.002\">erarbeitet</span><span style=\"opacity: 0.80\"> die </span><span style=\"background-color: hsl(0, 100.00%, 95.77%); opacity: 0.81\" title=\"-0.013\">bundesregier</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 91.88%); opacity: 0.82\" title=\"0.034\">derzeit</span><span style=\"opacity: 0.80\"> ein </span><span style=\"background-color: hsl(120, 100.00%, 94.86%); opacity: 0.81\" title=\"0.018\">gesetzentwurf</span><span style=\"opacity: 0.80\"> , der sich </span><span style=\"background-color: hsl(0, 100.00%, 93.93%); opacity: 0.81\" title=\"-0.022\">noch</span><span style=\"opacity: 0.80\"> in der </span><span style=\"background-color: hsl(0, 100.00%, 98.41%); opacity: 0.80\" title=\"-0.003\">abstimm</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 99.98%); opacity: 0.80\" title=\"0.000\">befindet</span><span style=\"opacity: 0.80\"> . </span><span style=\"background-color: hsl(120, 100.00%, 96.21%); opacity: 0.81\" title=\"0.011\">teil</span><span style=\"opacity: 0.80\"> der </span><span style=\"background-color: hsl(0, 100.00%, 97.76%); opacity: 0.80\" title=\"-0.005\">opposition</span><span style=\"opacity: 0.80\"> und </span><span style=\"background-color: hsl(120, 100.00%, 96.23%); opacity: 0.81\" title=\"0.011\">vor</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.20%); opacity: 0.80\" title=\"-0.001\">all</span><span style=\"opacity: 0.80\"> fluchtlingsorganisation </span><span style=\"background-color: hsl(0, 100.00%, 99.71%); opacity: 0.80\" title=\"-0.000\">kritisi</span><span style=\"opacity: 0.80\"> die </span><span style=\"background-color: hsl(120, 100.00%, 92.54%); opacity: 0.82\" title=\"0.030\">vorschlag</span><span style=\"opacity: 0.80\"> . </span><span style=\"background-color: hsl(0, 100.00%, 98.00%); opacity: 0.80\" title=\"-0.005\">hierzu</span><span style=\"opacity: 0.80\"> erklart der </span><span style=\"background-color: hsl(120, 100.00%, 87.51%); opacity: 0.84\" title=\"0.062\">innenpolit</span><span style=\"opacity: 0.80\"> sprech der cdu/csu-bundestagsfraktion </span><span style=\"background-color: hsl(120, 100.00%, 90.33%); opacity: 0.83\" title=\"0.043\">stephan</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.29%); opacity: 0.81\" title=\"0.011\">may</span><span style=\"opacity: 0.80\"> : `` die im koalitionsausschuss </span><span style=\"background-color: hsl(0, 100.00%, 99.47%); opacity: 0.80\" title=\"-0.001\">vereinbart</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 90.18%); opacity: 0.83\" title=\"0.044\">massnahm</span><span style=\"opacity: 0.80\"> sind die </span><span style=\"background-color: hsl(120, 100.00%, 93.30%); opacity: 0.82\" title=\"0.026\">richtig</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.02%); opacity: 0.81\" title=\"0.012\">reaktion</span><span style=\"opacity: 0.80\"> auf die </span><span style=\"background-color: hsl(0, 100.00%, 96.52%); opacity: 0.81\" title=\"-0.010\">dramat</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 99.98%); opacity: 0.80\" title=\"0.000\">steigend</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 89.84%); opacity: 0.83\" title=\"0.047\">fluchtlingszahl</span><span style=\"opacity: 0.80\"> . das zu ihr </span><span style=\"background-color: hsl(0, 100.00%, 99.09%); opacity: 0.80\" title=\"-0.001\">umsetz</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 91.88%); opacity: 0.82\" title=\"0.034\">derzeit</span><span style=\"opacity: 0.80\"> in der </span><span style=\"background-color: hsl(0, 100.00%, 98.41%); opacity: 0.80\" title=\"-0.003\">abstimm</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.77%); opacity: 0.80\" title=\"-0.000\">befind</span><span style=\"opacity: 0.80\"> gesetzespaket </span><span style=\"background-color: hsl(0, 100.00%, 97.47%); opacity: 0.80\" title=\"-0.006\">setzt</span><span style=\"opacity: 0.80\"> an den </span><span style=\"background-color: hsl(120, 100.00%, 93.30%); opacity: 0.82\" title=\"0.026\">richtig</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.23%); opacity: 0.80\" title=\"-0.001\">punkt</span><span style=\"opacity: 0.80\"> an . die </span><span style=\"background-color: hsl(0, 100.00%, 98.51%); opacity: 0.80\" title=\"-0.003\">kritik</span><span style=\"opacity: 0.80\"> an den </span><span style=\"background-color: hsl(120, 100.00%, 90.18%); opacity: 0.83\" title=\"0.044\">massnahm</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.22%); opacity: 0.81\" title=\"0.011\">verkennt</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.93%); opacity: 0.81\" title=\"0.013\">sowohl</span><span style=\"opacity: 0.80\"> die </span><span style=\"background-color: hsl(0, 100.00%, 98.50%); opacity: 0.80\" title=\"-0.003\">herausforder</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.23%); opacity: 0.81\" title=\"0.011\">vor</span><span style=\"opacity: 0.80\"> der </span><span style=\"background-color: hsl(120, 100.00%, 88.56%); opacity: 0.83\" title=\"0.055\">deutschland</span><span style=\"opacity: 0.80\"> und </span><span style=\"background-color: hsl(120, 100.00%, 96.22%); opacity: 0.81\" title=\"0.011\">europa</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 97.93%); opacity: 0.80\" title=\"0.005\">steh</span><span style=\"opacity: 0.80\"> als auch den </span><span style=\"background-color: hsl(0, 100.00%, 97.98%); opacity: 0.80\" title=\"-0.005\">sinn</span><span style=\"opacity: 0.80\"> des </span><span style=\"background-color: hsl(120, 100.00%, 88.81%); opacity: 0.83\" title=\"0.053\">asylrecht</span><span style=\"opacity: 0.80\"> . im </span><span style=\"background-color: hsl(120, 100.00%, 99.01%); opacity: 0.80\" title=\"0.002\">vordergrund</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 97.40%); opacity: 0.80\" title=\"-0.007\">steht</span><span style=\"opacity: 0.80\"> die </span><span style=\"background-color: hsl(120, 100.00%, 99.09%); opacity: 0.80\" title=\"0.001\">klar</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 94.76%); opacity: 0.81\" title=\"0.018\">unterscheid</span><span style=\"opacity: 0.80\"> von </span><span style=\"background-color: hsl(120, 100.00%, 87.29%); opacity: 0.84\" title=\"0.064\">schutzbedurft</span><span style=\"opacity: 0.80\"> und </span><span style=\"background-color: hsl(120, 100.00%, 93.79%); opacity: 0.81\" title=\"0.023\">mensch</span><span style=\"opacity: 0.80\"> , die </span><span style=\"background-color: hsl(120, 100.00%, 94.92%); opacity: 0.81\" title=\"0.017\">aus</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 91.46%); opacity: 0.82\" title=\"-0.036\">wirtschaft</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 94.98%); opacity: 0.81\" title=\"0.017\">grund</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.70%); opacity: 0.81\" title=\"0.009\">nach</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 88.56%); opacity: 0.83\" title=\"0.055\">deutschland</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.74%); opacity: 0.81\" title=\"0.013\">woll</span><span style=\"opacity: 0.80\"> . zu </span><span style=\"background-color: hsl(0, 100.00%, 99.42%); opacity: 0.80\" title=\"-0.001\">letzt</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.56%); opacity: 0.80\" title=\"-0.001\">gehor</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.23%); opacity: 0.81\" title=\"0.011\">vor</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.20%); opacity: 0.80\" title=\"-0.001\">all</span><span style=\"opacity: 0.80\"> die </span><span style=\"background-color: hsl(120, 100.00%, 93.79%); opacity: 0.81\" title=\"0.023\">mensch</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 94.92%); opacity: 0.81\" title=\"0.017\">aus</span><span style=\"opacity: 0.80\"> dem westbalkan . wir muss </span><span style=\"background-color: hsl(120, 100.00%, 95.37%); opacity: 0.81\" title=\"0.015\">deshalb</span><span style=\"opacity: 0.80\"> die fehlanreiz fur asylfremd </span><span style=\"background-color: hsl(120, 100.00%, 82.58%); opacity: 0.86\" title=\"0.100\">migration</span><span style=\"opacity: 0.80\"> und die </span><span style=\"background-color: hsl(0, 100.00%, 97.93%); opacity: 0.80\" title=\"-0.005\">freie</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 95.25%); opacity: 0.81\" title=\"-0.016\">wahl</span><span style=\"opacity: 0.80\"> des </span><span style=\"background-color: hsl(120, 100.00%, 95.79%); opacity: 0.81\" title=\"0.013\">aufnahmeland</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 94.60%); opacity: 0.81\" title=\"0.019\">konsequent</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.55%); opacity: 0.81\" title=\"0.010\">verhind</span><span style=\"opacity: 0.80\"> . </span><span style=\"background-color: hsl(120, 100.00%, 95.60%); opacity: 0.81\" title=\"0.014\">nur</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 97.01%); opacity: 0.80\" title=\"-0.008\">so</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.72%); opacity: 0.81\" title=\"0.023\">konn</span><span style=\"opacity: 0.80\"> wir die </span><span style=\"background-color: hsl(120, 100.00%, 97.86%); opacity: 0.80\" title=\"0.005\">handlungsfah</span><span style=\"opacity: 0.80\"> des asylsystem </span><span style=\"background-color: hsl(0, 100.00%, 96.94%); opacity: 0.81\" title=\"-0.008\">zugun</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 94.74%); opacity: 0.81\" title=\"0.018\">wirklich</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 87.29%); opacity: 0.84\" title=\"0.064\">schutzbedurft</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.89%); opacity: 0.81\" title=\"0.013\">erhalt</span><span style=\"opacity: 0.80\"> . auch ein </span><span style=\"background-color: hsl(0, 100.00%, 91.46%); opacity: 0.82\" title=\"-0.036\">wirtschaft</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 98.07%); opacity: 0.80\" title=\"-0.004\">stark</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 89.96%); opacity: 0.83\" title=\"0.046\">land</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 97.29%); opacity: 0.80\" title=\"-0.007\">wie</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 88.56%); opacity: 0.83\" title=\"0.055\">deutschland</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 92.81%); opacity: 0.82\" title=\"0.028\">kann</span><span style=\"opacity: 0.80\"> nicht </span><span style=\"background-color: hsl(0, 100.00%, 99.20%); opacity: 0.80\" title=\"-0.001\">all</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 91.96%); opacity: 0.82\" title=\"0.033\">aufnehm</span><span style=\"opacity: 0.80\"> , die sich ein </span><span style=\"background-color: hsl(120, 100.00%, 90.60%); opacity: 0.83\" title=\"0.042\">bess</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 92.09%); opacity: 0.82\" title=\"0.033\">leb</span><span style=\"opacity: 0.80\"> erhoff und </span><span style=\"background-color: hsl(120, 100.00%, 94.92%); opacity: 0.81\" title=\"0.017\">aus</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.31%); opacity: 0.81\" title=\"0.011\">person</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 94.98%); opacity: 0.81\" title=\"0.017\">grund</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 97.51%); opacity: 0.80\" title=\"-0.006\">gerad</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.70%); opacity: 0.81\" title=\"0.009\">nach</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 88.56%); opacity: 0.83\" title=\"0.055\">deutschland</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.74%); opacity: 0.81\" title=\"0.013\">woll</span><span style=\"opacity: 0.80\"> . </span><span style=\"background-color: hsl(120, 100.00%, 90.54%); opacity: 0.83\" title=\"0.042\">uns</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.84%); opacity: 0.81\" title=\"0.009\">bevolker</span><span style=\"opacity: 0.80\"> hat ein </span><span style=\"background-color: hsl(120, 100.00%, 93.15%); opacity: 0.82\" title=\"0.027\">anspruch</span><span style=\"opacity: 0.80\"> auf sich und </span><span style=\"background-color: hsl(120, 100.00%, 94.62%); opacity: 0.81\" title=\"0.019\">geordnet</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 96.63%); opacity: 0.81\" title=\"-0.010\">verhaltnis</span><span style=\"opacity: 0.80\"> . wir muss </span><span style=\"background-color: hsl(120, 100.00%, 95.37%); opacity: 0.81\" title=\"0.015\">deshalb</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.55%); opacity: 0.81\" title=\"0.014\">darauf</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.08%); opacity: 0.81\" title=\"0.016\">acht</span><span style=\"opacity: 0.80\"> , dass die </span><span style=\"background-color: hsl(120, 100.00%, 91.70%); opacity: 0.82\" title=\"0.035\">humanitar</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 97.02%); opacity: 0.80\" title=\"0.008\">hilf</span><span style=\"opacity: 0.80\"> , die </span><span style=\"background-color: hsl(120, 100.00%, 88.56%); opacity: 0.83\" title=\"0.055\">deutschland</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.55%); opacity: 0.81\" title=\"0.024\">leistet</span><span style=\"opacity: 0.80\"> , an den </span><span style=\"background-color: hsl(120, 100.00%, 93.30%); opacity: 0.82\" title=\"0.026\">richtig</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 97.93%); opacity: 0.80\" title=\"-0.005\">stell</span><span style=\"opacity: 0.80\"> ansetzt . wir </span><span style=\"background-color: hsl(120, 100.00%, 95.50%); opacity: 0.81\" title=\"0.015\">hab</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 98.74%); opacity: 0.80\" title=\"0.002\">ausserd</span><span style=\"opacity: 0.80\"> ein </span><span style=\"background-color: hsl(120, 100.00%, 96.72%); opacity: 0.81\" title=\"0.009\">verpflicht</span><span style=\"opacity: 0.80\"> , rechtstaat </span><span style=\"background-color: hsl(120, 100.00%, 93.69%); opacity: 0.81\" title=\"0.024\">verfahr</span><span style=\"opacity: 0.80\"> zu </span><span style=\"background-color: hsl(120, 100.00%, 96.85%); opacity: 0.81\" title=\"0.009\">gewahrleist</span><span style=\"opacity: 0.80\"> . </span><span style=\"background-color: hsl(0, 100.00%, 98.82%); opacity: 0.80\" title=\"-0.002\">gleichzeit</span><span style=\"opacity: 0.80\"> muss wir </span><span style=\"background-color: hsl(0, 100.00%, 99.20%); opacity: 0.80\" title=\"-0.001\">all</span><span style=\"opacity: 0.80\"> fur die </span><span style=\"background-color: hsl(120, 100.00%, 64.18%); opacity: 0.97\" title=\"0.281\">integration</span><span style=\"opacity: 0.80\"> der </span><span style=\"background-color: hsl(120, 100.00%, 93.79%); opacity: 0.81\" title=\"0.023\">mensch</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 95.24%); opacity: 0.81\" title=\"-0.016\">tun</span><span style=\"opacity: 0.80\"> , die </span><span style=\"background-color: hsl(120, 100.00%, 89.78%); opacity: 0.83\" title=\"0.047\">dauerhaft</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 90.13%); opacity: 0.83\" title=\"0.045\">hier</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 94.28%); opacity: 0.81\" title=\"0.020\">bleib</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 90.52%); opacity: 0.83\" title=\"0.042\">durf</span><span style=\"opacity: 0.80\"> . </span><span style=\"background-color: hsl(120, 100.00%, 88.56%); opacity: 0.83\" title=\"0.055\">deutschland</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.85%); opacity: 0.81\" title=\"0.009\">nimmt</span><span style=\"opacity: 0.80\"> in der </span><span style=\"background-color: hsl(120, 100.00%, 85.65%); opacity: 0.85\" title=\"0.076\">eu</span><span style=\"opacity: 0.80\"> mit </span><span style=\"background-color: hsl(120, 100.00%, 94.45%); opacity: 0.81\" title=\"0.020\">abstand</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 96.06%); opacity: 0.81\" title=\"-0.012\">am</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.00%); opacity: 0.81\" title=\"0.017\">meist</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 60.00%); opacity: 1.00\" title=\"0.330\">fluchtling</span><span style=\"opacity: 0.80\"> auf . es ist </span><span style=\"background-color: hsl(120, 100.00%, 97.00%); opacity: 0.80\" title=\"0.008\">aber</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 98.90%); opacity: 0.80\" title=\"0.002\">unrealist</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 94.51%); opacity: 0.81\" title=\"0.019\">anzunehm</span><span style=\"opacity: 0.80\"> , </span><span style=\"background-color: hsl(120, 100.00%, 88.56%); opacity: 0.83\" title=\"0.055\">deutschland</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.72%); opacity: 0.81\" title=\"0.023\">konn</span><span style=\"opacity: 0.80\"> die </span><span style=\"background-color: hsl(120, 100.00%, 99.82%); opacity: 0.80\" title=\"0.000\">aufgab</span><span style=\"opacity: 0.80\"> des fluchtlingsschutz </span><span style=\"background-color: hsl(120, 100.00%, 89.78%); opacity: 0.83\" title=\"0.047\">dauerhaft</span><span style=\"opacity: 0.80\"> fur die </span><span style=\"background-color: hsl(120, 100.00%, 96.44%); opacity: 0.81\" title=\"0.010\">mehrheit</span><span style=\"opacity: 0.80\"> der </span><span style=\"background-color: hsl(120, 100.00%, 95.66%); opacity: 0.81\" title=\"0.014\">and</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 85.65%); opacity: 0.85\" title=\"0.076\">eu</span><span style=\"opacity: 0.80\">-</span><span style=\"background-color: hsl(120, 100.00%, 93.48%); opacity: 0.81\" title=\"0.025\">staat</span><span style=\"opacity: 0.80\"> mitleist . </span><span style=\"background-color: hsl(120, 100.00%, 95.37%); opacity: 0.81\" title=\"0.015\">deshalb</span><span style=\"opacity: 0.80\"> ist die </span><span style=\"background-color: hsl(120, 100.00%, 93.98%); opacity: 0.81\" title=\"0.022\">festleg</span><span style=\"opacity: 0.80\"> des koalitionsausschuss </span><span style=\"background-color: hsl(120, 100.00%, 93.30%); opacity: 0.82\" title=\"0.026\">richtig</span><span style=\"opacity: 0.80\"> , sekundarmigration , d.h. die </span><span style=\"background-color: hsl(120, 100.00%, 82.58%); opacity: 0.86\" title=\"0.100\">migration</span><span style=\"opacity: 0.80\"> von ein </span><span style=\"background-color: hsl(120, 100.00%, 85.65%); opacity: 0.85\" title=\"0.076\">eu</span><span style=\"opacity: 0.80\">-</span><span style=\"background-color: hsl(120, 100.00%, 93.48%); opacity: 0.81\" title=\"0.025\">staat</span><span style=\"opacity: 0.80\"> in ein </span><span style=\"background-color: hsl(120, 100.00%, 95.66%); opacity: 0.81\" title=\"0.014\">and</span><span style=\"opacity: 0.80\"> , zu </span><span style=\"background-color: hsl(120, 100.00%, 96.55%); opacity: 0.81\" title=\"0.010\">verhind</span><span style=\"opacity: 0.80\"> . </span><span style=\"background-color: hsl(120, 100.00%, 95.66%); opacity: 0.81\" title=\"0.014\">and</span><span style=\"opacity: 0.80\"> hat ein lasten-</span><span style=\"background-color: hsl(120, 100.00%, 87.17%); opacity: 0.84\" title=\"0.065\">verteil</span><span style=\"opacity: 0.80\"> in der </span><span style=\"background-color: hsl(120, 100.00%, 85.65%); opacity: 0.85\" title=\"0.076\">eu</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 94.53%); opacity: 0.81\" title=\"0.019\">kein</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 97.98%); opacity: 0.80\" title=\"-0.005\">sinn</span><span style=\"opacity: 0.80\"> . es </span><span style=\"background-color: hsl(120, 100.00%, 92.81%); opacity: 0.82\" title=\"0.028\">kann</span><span style=\"opacity: 0.80\"> nicht </span><span style=\"background-color: hsl(120, 100.00%, 93.30%); opacity: 0.82\" title=\"0.026\">richtig</span><span style=\"opacity: 0.80\"> sein , </span><span style=\"background-color: hsl(120, 100.00%, 92.95%); opacity: 0.82\" title=\"0.028\">wenn</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 60.00%); opacity: 1.00\" title=\"0.330\">fluchtling</span><span style=\"opacity: 0.80\"> sich aussuch , in </span><span style=\"background-color: hsl(0, 100.00%, 97.85%); opacity: 0.80\" title=\"-0.005\">welch</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 85.65%); opacity: 0.85\" title=\"0.076\">eu</span><span style=\"opacity: 0.80\">-</span><span style=\"background-color: hsl(120, 100.00%, 89.96%); opacity: 0.83\" title=\"0.046\">land</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 97.64%); opacity: 0.80\" title=\"0.006\">sie</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 91.54%); opacity: 0.82\" title=\"0.036\">schutz</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 97.01%); opacity: 0.80\" title=\"0.008\">find</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.41%); opacity: 0.81\" title=\"0.015\">mocht</span><span style=\"opacity: 0.80\"> . </span><span style=\"background-color: hsl(0, 100.00%, 96.23%); opacity: 0.81\" title=\"-0.011\">vom</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.49%); opacity: 0.81\" title=\"0.010\">grundsatz</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 98.87%); opacity: 0.80\" title=\"-0.002\">her</span><span style=\"opacity: 0.80\"> muss </span><span style=\"background-color: hsl(120, 100.00%, 99.09%); opacity: 0.80\" title=\"0.001\">klar</span><span style=\"opacity: 0.80\"> sein , dass es </span><span style=\"background-color: hsl(120, 100.00%, 60.00%); opacity: 1.00\" title=\"0.330\">fluchtling</span><span style=\"opacity: 0.80\"> zumutbar ist , in </span><span style=\"background-color: hsl(0, 100.00%, 99.20%); opacity: 0.80\" title=\"-0.001\">all</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 85.65%); opacity: 0.85\" title=\"0.076\">eu</span><span style=\"opacity: 0.80\">-</span><span style=\"background-color: hsl(120, 100.00%, 84.93%); opacity: 0.85\" title=\"0.082\">mitgliedstaat</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 91.54%); opacity: 0.82\" title=\"0.036\">schutz</span><span style=\"opacity: 0.80\"> zu </span><span style=\"background-color: hsl(120, 100.00%, 97.01%); opacity: 0.80\" title=\"0.008\">find</span><span style=\"opacity: 0.80\"> . ein </span><span style=\"background-color: hsl(120, 100.00%, 94.21%); opacity: 0.81\" title=\"0.021\">wahlrecht</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 92.81%); opacity: 0.82\" title=\"0.028\">kann</span><span style=\"opacity: 0.80\"> es nicht </span><span style=\"background-color: hsl(120, 100.00%, 99.28%); opacity: 0.80\" title=\"0.001\">geb</span><span style=\"opacity: 0.80\"> . ein </span><span style=\"background-color: hsl(120, 100.00%, 97.52%); opacity: 0.80\" title=\"0.006\">gerecht</span><span style=\"opacity: 0.80\"> und </span><span style=\"background-color: hsl(120, 100.00%, 95.98%); opacity: 0.81\" title=\"0.012\">solidar</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 87.17%); opacity: 0.84\" title=\"0.065\">verteil</span><span style=\"opacity: 0.80\"> von </span><span style=\"background-color: hsl(120, 100.00%, 60.00%); opacity: 1.00\" title=\"0.330\">fluchtling</span><span style=\"opacity: 0.80\"> in </span><span style=\"background-color: hsl(120, 100.00%, 98.28%); opacity: 0.80\" title=\"0.004\">erganz</span><span style=\"opacity: 0.80\"> des </span><span style=\"background-color: hsl(120, 100.00%, 94.46%); opacity: 0.81\" title=\"0.020\">geltend</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 89.23%); opacity: 0.83\" title=\"0.051\">dublin</span><span style=\"opacity: 0.80\">-</span><span style=\"background-color: hsl(120, 100.00%, 97.87%); opacity: 0.80\" title=\"0.005\">system</span><span style=\"opacity: 0.80\"> , muss auch </span><span style=\"background-color: hsl(120, 100.00%, 97.41%); opacity: 0.80\" title=\"0.007\">praktisch</span><span style=\"opacity: 0.80\"> umsetzbar sein . ein </span><span style=\"background-color: hsl(120, 100.00%, 92.64%); opacity: 0.82\" title=\"0.029\">ansatz</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.17%); opacity: 0.81\" title=\"0.016\">hierfur</span><span style=\"opacity: 0.80\"> sind die </span><span style=\"background-color: hsl(0, 100.00%, 95.36%); opacity: 0.81\" title=\"-0.015\">geplant</span><span style=\"opacity: 0.80\"> leistungskurz . &#x27;&#x27; </span><span style=\"background-color: hsl(120, 100.00%, 99.01%); opacity: 0.80\" title=\"0.002\">pressekontakt</span><span style=\"opacity: 0.80\"> : cdu/csu - bundestagsfraktion pressestell telefon : ( 030 ) 227-52360 fax : ( 030 ) 227-56660 internet : email : original-content von : cdu/csu - bundestagsfraktion , ubermittelt durch news aktuell </span>\n",
       "    </p>\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example text\n",
    "eli5.show_prediction(logreg, test[0][11][5], vec = vec_tfidf_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visual inspection of the best predictor words underlines that the classifier does what it should. The best predictor words are all strongly related to migration. It does not seem like we have to worry about overfitting.\n",
    "\n",
    "Lastly, the uncoded cases are predicted and everything is written into a single csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictset = []\n",
    "with open(basedir+'intermediate/predict.csv', mode=\"r\", encoding=\"utf-8\") as predict:\n",
    "    reader_predict = csv.reader(predict)\n",
    "    for row in reader_predict:\n",
    "        predictset.append(row[5])\n",
    "\n",
    "pred_vec  = vec_tfidf_f.transform(predictset)\n",
    "pred_yhat = logreg.predict(pred_vec)\n",
    "\n",
    "with open(basedir + 'final_dataset.csv', mode=\"w\", encoding=\"utf-8\") as fo:\n",
    "    with open(basedir + 'intermediate/train.csv', mode=\"r\", encoding=\"utf-8\") as train:\n",
    "        with open(basedir + 'intermediate/test.csv', mode=\"r\", encoding=\"utf-8\") as test:\n",
    "            with open(basedir + 'intermediate/predict.csv', mode=\"r\", encoding=\"utf-8\") as predict:\n",
    "                \n",
    "                fieldnames = ['date', 'sender', 'title', 'link', 'raw', 'clean_full', 'clean_rest', 'coding']\n",
    "                writer = csv.DictWriter(fo, lineterminator = '\\n', fieldnames = fieldnames)\n",
    "                writer.writeheader()\n",
    "                \n",
    "                reader_train   = csv.reader(train)\n",
    "                reader_test    = csv.reader(test)\n",
    "                reader_predict = csv.reader(predict)\n",
    "                next(reader_train)\n",
    "                \n",
    "                # append prediction to uncoded cases, write to csv\n",
    "                for row, p in zip(reader_predict, pred_yhat):\n",
    "                    writer.writerow({'date':         row[0], \n",
    "                                     'sender':       row[1], \n",
    "                                     'title':        row[2], \n",
    "                                     'link':         row[3],\n",
    "                                     'raw':          row[4],\n",
    "                                     'clean_full':   row[5],\n",
    "                                     'clean_rest':   row[6], \n",
    "                                     'coding':       p})\n",
    "                        \n",
    "                # add training set\n",
    "                for row in reader_train:\n",
    "                    writer.writerow({'date':         row[0], \n",
    "                                     'sender':       row[1], \n",
    "                                     'title':        row[2], \n",
    "                                     'link':         row[3],\n",
    "                                     'raw':          row[4],\n",
    "                                     'clean_full':   row[5],\n",
    "                                     'clean_rest':   row[6], \n",
    "                                     'coding':       row[7]})\n",
    "                \n",
    "                # add test set\n",
    "                for row in reader_test:\n",
    "                    writer.writerow({'date':         row[0], \n",
    "                                     'sender':       row[1], \n",
    "                                     'title':        row[2], \n",
    "                                     'link':         row[3],\n",
    "                                     'raw':          row[4],\n",
    "                                     'clean_full':   row[5],\n",
    "                                     'clean_rest':   row[6], \n",
    "                                     'coding':       row[7]})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04 Handcoding the data to train a classifier\n",
    "\n",
    "In order to train a classifier, I need a pre-defined classification. This means that I will have to code a selection by hand. First I import necessary packages and define a string for  a base directory, which prevents extensive typing/copy-paste efforts when defining directories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "basedir = 'C:/Users/samunico/OneDrive/Dokumente/Studium/Amsterdam/Studies/Semester 2/Block I/Big Data/FinalPaper/Releases/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I randomly sample 1000 cases from the initial data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_cases = random.sample(range(29536), 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I could simply subset the data, write it to a csv and then by hand code zeros and ones in a new row. However, I could also make this task less error prone and less tedious writing some code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Is this text about immigration?\n",
      "\n",
      " Text 1/1000:\n",
      "\n",
      "\n",
      "Brand: Verfolgte Christin Asia Bibi endlich in Sicherheit\n",
      "\n",
      "[y]es\n",
      "[n]o\n",
      "[m]ore?\n"
     ]
    }
   ],
   "source": [
    "dt = str(datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "# open csv with all cases\n",
    "with open(basedir + 'Releases20190516-190649.csv', mode=\"r\", encoding=\"utf-8\") as fi:\n",
    "    # open new csv to write coded cases to\n",
    "    with open(basedir + \"Coded\"+dt+\".csv\",mode=\"w\", encoding=\"utf-8\") as fo:    \n",
    "        reader = csv.reader(fi)\n",
    "        writer = csv.writer(fo, lineterminator='\\n')\n",
    "        \n",
    "        # define subsample from full set\n",
    "        codesample = [row for idx, row in enumerate(reader) if idx in code_cases]\n",
    "        \n",
    "        # set up counter\n",
    "        i = 1\n",
    "        for row in codesample:\n",
    "            input_val = False\n",
    "            # as long as no correct input was given, keep asking\n",
    "            while input_val == False:\n",
    "                \n",
    "                # question\n",
    "                print('\\nIs this text about immigration?\\n\\n Text ' + str(i) + '/1000:\\n\\n')\n",
    "                \n",
    "                # print press release title\n",
    "                print(row[2])\n",
    "                print('\\n[y]es\\n[n]o\\n[m]ore?')\n",
    "                \n",
    "                # get keyboard input\n",
    "                code1 = input()\n",
    "                \n",
    "                # if input == y, append 1\n",
    "                if code1 == 'y':\n",
    "                    row.append(1)\n",
    "                    writer.writerow(row)\n",
    "                    i += 1\n",
    "                    input_val = True\n",
    "                \n",
    "                # append 0 for n\n",
    "                elif code1 ==  'n':\n",
    "                    row.append(0)\n",
    "                    writer.writerow(row)\n",
    "                    i += 1\n",
    "                    input_val = True\n",
    "                \n",
    "                # if input is m, print press release content for further inspection and ask again for classification\n",
    "                elif code1 == 'm':\n",
    "                    print('\\nIs this text about immigration?\\n\\nText:\\n\\n')\n",
    "                    print(row[2] + '\\n\\n' + row[4])\n",
    "                    print('\\n[y]es\\n[n]o?')\n",
    "                    code2 = input()\n",
    "                    if code2 == 'y':\n",
    "                        row.append(1)\n",
    "                        writer.writerow(row)\n",
    "                        i += 1\n",
    "                        input_val = True\n",
    "                    elif code2 ==  'n':\n",
    "                        row.append(0)\n",
    "                        writer.writerow(row)\n",
    "                        i += 1\n",
    "                        input_val = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the related url, I bind the coded cases to the original data again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Coded20190518-132350.csv\",mode=\"r\", encoding=\"utf-8\") as fi:\n",
    "    reader = csv.reader(fi)\n",
    "    positive = []\n",
    "    negative = []\n",
    "    for row in reader:\n",
    "        if row[5] == '1':\n",
    "            positive.append(row[3])\n",
    "        elif row[5] == '0':\n",
    "            negative.append(row[3])\n",
    "        else:\n",
    "            print('Error')\n",
    "            break\n",
    "\n",
    "dt = str(datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "with open(basedir + 'Cleaned20190520-122409.csv', mode=\"r\", encoding=\"utf-8\") as fi:\n",
    "    with open(basedir + 'Cleaned_coded' + dt + '.csv', mode=\"w\", encoding=\"utf-8\") as fo:\n",
    "        reader = csv.reader(fi)\n",
    "        fieldnames = ['date', 'sender', 'title', 'link', 'raw', 'clean_full', 'clean_rest', 'coding']\n",
    "        writer = csv.DictWriter(fo, lineterminator='\\n', fieldnames = fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in reader:\n",
    "            if row[3] in positive:\n",
    "                row.append(1)\n",
    "            elif row[3] in negative:\n",
    "                row.append(0)\n",
    "            else:\n",
    "                row.append('')    \n",
    "            writer.writerow({'date':        row[0], \n",
    "                            'sender':       row[1],\n",
    "                            'title':        row[2], \n",
    "                            'link':         row[3], \n",
    "                            'raw':          row[4], \n",
    "                            'clean_full':   row[5], \n",
    "                            'clean_rest':   row[6],\n",
    "                            'coding':       row[7]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the data: how many press releases in our set are concerned with immigration?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    }
   ],
   "source": [
    "with open(basedir + 'Coded20190518-132350.csv', mode = 'r', encoding = 'utf-8') as fi:\n",
    "    reader = csv.reader(fi)\n",
    "    i = 0\n",
    "    for row in reader:\n",
    "        if row[5] == '1':\n",
    "            i += 1\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh no - only 29 out of 1000 cases are related to immigration! This unbalanced sample won't be sufficient to train the classifier. However, it is not like we do not get any information from this data. I can use it to train a classifier to make an informed decision which cases to inspect in order to oversample press releases related to immigration from the full dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do this, I first need to create a balanced sample and vectorize the texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# define train and predict set\n",
    "train = []\n",
    "predict = []\n",
    "\n",
    "with open(basedir + 'Cleaned_coded20190520-151658.csv', mode=\"r\", encoding=\"utf-8\") as fi:\n",
    "    reader = csv.reader(fi)\n",
    "    for row in reader:\n",
    "        if row[7] == '1':\n",
    "            train.append(row)\n",
    "        elif row[7] == '0':\n",
    "            train.append(row)\n",
    "        predict.append(row[5])\n",
    "\n",
    "# define x and y for the classifier:\n",
    "texts  = [t[5] for t in train]\n",
    "predictor = [int(t[7]) for t in train]\n",
    "sum(predictor) # 29 positive cases -> lets use a similar number of non-cases for the algorithm\n",
    "\n",
    "# undersample majority class (= not immigration) to get a balanced sample\n",
    "rsample = random.sample(range(100), 70)\n",
    "texts_us = []\n",
    "predictor_us = []\n",
    "i = 0\n",
    "for p,t in zip(predictor, texts):\n",
    "    if p == 1:\n",
    "        predictor_us.append(p)\n",
    "        texts_us.append(t)\n",
    "        i += 1\n",
    "    if p == 0:\n",
    "        if i in rsample:\n",
    "           predictor_us.append(p)\n",
    "           texts_us.append(t)\n",
    "           i += 1 \n",
    "        \n",
    "\n",
    "\n",
    "# vectorize texts\n",
    "vec_count = CountVectorizer(max_df=.5, min_df=5)\n",
    "count = vec_count.fit_transform(texts_us)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train a model to identify most-likely cases for subsequent hand-coding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samunico\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8302996344628804\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# define and fit model\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(count, predictor_us)\n",
    "\n",
    "#%% identify most-likely immigration cases by classifying all uncoded text\n",
    "count_f = vec_count.transform(predict[1:]) # [1:] excludes header\n",
    "probability = logreg.predict_proba(count_f) # lets take the proability to identify most likely cases\n",
    "print(sorted([p[1] for p in probability])[-500]) # return value of the 500th-most-likely case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this threshold provided by our classifier, I can code 500 more cases, but this time they are far more likely to be about immigration, which should result in a more balanced sample. To be sure, I write the output of this preliminary classifier, as well as the subsample of most-likely-cases for hand-coding into csv-files so I don't lose them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2 = []\n",
    "dt = str(datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "with open(basedir + 'Cleaned_coded20190520-151658.csv', mode=\"r\", encoding=\"utf-8\") as fi:\n",
    "    with open(basedir + 'Cleaned_prelim.csv', mode=\"w\", encoding=\"utf-8\") as fo:\n",
    "        reader = csv.reader(fi)\n",
    "        fieldnames = ['date', 'sender', 'title', 'link', 'raw', 'clean_full', 'clean_rest', 'coding', 'probability']\n",
    "        writer = csv.DictWriter(fo, lineterminator='\\n', fieldnames = fieldnames)\n",
    "        writer.writeheader()\n",
    "        i = 0\n",
    "        next(reader)  # Skip header row.\n",
    "        for row in reader:\n",
    "            row.append(round(probability[i][1], 2))\n",
    "            writer.writerow({'date':        row[0], \n",
    "                            'sender':       row[1],\n",
    "                            'title':        row[2], \n",
    "                            'link':         row[3], \n",
    "                            'raw':          row[4], \n",
    "                            'clean_full':   row[5], \n",
    "                            'clean_rest':   row[6],\n",
    "                            'coding':       row[7],\n",
    "                            'probability':  row[8]})\n",
    "            i += 1\n",
    "            if row[7] != '0' and row[7] != '1': # check whether cases are pre-coded \n",
    "                if float(row[8]) > .86: # only include 500 most likely cases\n",
    "                    train2.append([row[2], row[3], row[4]]) # extract title, link and raw text\n",
    "\n",
    "with open(basedir + 'oversample.csv', mode=\"w\", encoding=\"utf-8\") as fo:\n",
    "    writer = csv.writer(fo, lineterminator='\\n')\n",
    "    for row in train2:\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, I use a program for the handcoding to prevent errors from skipping a row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Is this text about immigration?\n",
      "\n",
      " Text 0/495:\n",
      "\n",
      "\n",
      "Gröhe: Dankbar für den Einsatz der Kirchen bei der Suizidprävention\n",
      "\n",
      "[y]es\n",
      "[n]o\n",
      "[m]ore?\n",
      "n\n",
      "\n",
      "Is this text about immigration?\n",
      "\n",
      " Text 1/495:\n",
      "\n",
      "\n",
      "Brand: Humanitäre Hilfe für Syrien und Nachbarländer sorgt für mehr Stabilität\n",
      "\n",
      "[y]es\n",
      "[n]o\n",
      "[m]ore?\n",
      "m\n",
      "\n",
      "Is this text about immigration?\n",
      "\n",
      "Text:\n",
      "\n",
      "\n",
      "Brand: Humanitäre Hilfe für Syrien und Nachbarländer sorgt für mehr Stabilität\n",
      "\n",
      "14.03.2019 – 16:28                                               Deutschland kommt seiner Verantwortung nach Die Bundesregierung sagt auf der Syrien-Konferenz in Brüssel 1,44 Milliarden Euro für humanitäre Hilfe und entwicklungsorientierte Maßnahmen in den Nachbarländern zu. Dazu erklärt der Vorsitzende der Arbeitsgruppe Menschenrechte und humanitäre Hilfe der CDU/CSU-Bundestagsfraktion, Michael Brand: \"Mit der Zusage Deutschlands, den Betrag für die humanitäre Hilfe in Syrien und den Nachbarländern erneut zu erhöhen, kommt Deutschland als zweitgrößter internationaler Geber seiner humanitären Verpflichtung nach. Die Menschen, die in dem seit über acht Jahren vom Krieg auf das Äußerste strapazierten Land leben, und diejenigen, die in die Nachbarländer geflüchtet sind, werden nicht vergessen. Nach Angaben der Vereinten Nationen sind rund 11,7 Millionen Menschen innerhalb Syriens auf humanitäre Hilfe angewiesen, 6,2 Millionen sind Vertriebene im eigenen Land. Weitere 5,7 Millionen Syrer haben ihre Heimat verlassen, allein der Libanon hat 1,2 Millionen syrische Flüchtlinge aufgenommen. Besonders dramatisch ist die Situation der Kinder. Das Kinderhilfswerk der Vereinten Nationen verweist darauf, dass allein im vergangenen Jahr mehr Kinder getötet worden sind als in den Jahren zuvor in diesem Konflikt. Die Staatengemeinschaft bleibt dringend aufgefordert, sich für die humanitäre Hilfe zu engagieren, da derzeit nur zehn Geberländer 90 Prozent der humanitären Hilfe leisten. Der steigenden Anzahl von Konflikten, die auch immer länger dauern, kann nur gemeinschaftlich begegnet werden. Wesentlich ist - neben der Sicherung des dringend zum Überleben Benötigten - der feste Wille der Staatengemeinschaft, endlich eine politische Lösung des Konflikts herbeizuführen, damit die Menschen in ihre Heimat zurückkehren können.\" Pressekontakt: CDU/CSU - Bundestagsfraktion Pressestelle Telefon:  (030) 227-52360 Fax:      (030) 227-56660 Internet:  Email: Original-Content von: CDU/CSU - Bundestagsfraktion, übermittelt durch news aktuell\n",
      "\n",
      "[y]es\n",
      "[n]o?\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "with open(basedir + 'oversample.csv', mode=\"r\", encoding=\"utf-8\") as fi:\n",
    "    with open(basedir + 'oversample_coded.csv', mode=\"w\", encoding=\"utf-8\") as fo:           \n",
    "        reader = csv.reader(fi)\n",
    "        writer = csv.writer(fo, lineterminator='\\n')\n",
    "        for row in reader:\n",
    "            input_val = False\n",
    "            while input_val == False:\n",
    "                print('\\nIs this text about immigration?\\n\\n Text ' + str(i) + '/495:\\n\\n')\n",
    "                print(row[0])\n",
    "                print('\\n[y]es\\n[n]o\\n[m]ore?')\n",
    "                code1 = input()\n",
    "                if code1 == 'y':\n",
    "                    row.append(1)\n",
    "                    i += 1\n",
    "                    input_val = True\n",
    "                elif code1 ==  'n':\n",
    "                    row.append(0)\n",
    "                    i += 1\n",
    "                    input_val = True\n",
    "                elif code1 == 'm':\n",
    "                    print('\\nIs this text about immigration?\\n\\nText:\\n\\n')\n",
    "                    print(row[0] + '\\n\\n' + row[2])\n",
    "                    print('\\n[y]es\\n[n]o?')\n",
    "                    code2 = input()\n",
    "                    if code2 == 'y':\n",
    "                        row.append(1)\n",
    "                        i += 1\n",
    "                        input_val = True\n",
    "                    elif code2 ==  'n':\n",
    "                        row.append(0)\n",
    "                        i += 1\n",
    "                        input_val = True\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I write them to the full csv again (I am fully aware that this is very inefficient, and should be done within a loop, but I lack the time to change this now):  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(basedir + \"oversample_final.csv\",mode=\"r\", encoding=\"utf-8\") as fi:\n",
    "    reader = csv.reader(fi)\n",
    "    positive = []\n",
    "    negative = []\n",
    "    for row in reader:\n",
    "        if row[3] == '1':\n",
    "            positive.append(row[1])\n",
    "        elif row[3] == '0':\n",
    "            negative.append(row[1])\n",
    "        else:\n",
    "            print('Error')\n",
    "            break\n",
    "\n",
    "with open('C:/Users/samunico/OneDrive/Dokumente/Studium/Amsterdam/Studies/Semester 2/Block I/Big Data/FinalPaper/Releases/Cleaned_coded20190520-151658.csv', mode=\"r\", encoding=\"utf-8\") as fi:\n",
    "    with open('C:/Users/samunico/OneDrive/Dokumente/Studium/Amsterdam/Studies/Semester 2/Block I/Big Data/FinalPaper/Releases/full_coded.csv', mode=\"w\", encoding=\"utf-8\") as fo:\n",
    "        reader = csv.reader(fi)\n",
    "        fieldnames = ['date', 'sender', 'title', 'link', 'raw', 'clean_full', 'clean_rest', 'coding']\n",
    "        writer = csv.DictWriter(fo, lineterminator='\\n', fieldnames = fieldnames)\n",
    "        writer.writeheader()\n",
    "        next(reader)\n",
    "        for row in reader:\n",
    "            if row[3] in positive:\n",
    "                row[7] = 1\n",
    "            elif row[3] in negative:\n",
    "                row[7] = 0    \n",
    "            writer.writerow({'date':        row[0], \n",
    "                            'sender':       row[1],\n",
    "                            'title':        row[2], \n",
    "                            'link':         row[3], \n",
    "                            'raw':          row[4], \n",
    "                            'clean_full':   row[5], \n",
    "                            'clean_rest':   row[6],\n",
    "                            'coding':       row[7]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did my method work? How many of the cases are positive?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196\n"
     ]
    }
   ],
   "source": [
    "print(len(positive))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "196/500! This should be sufficient to work with. With a little oversampling, I should be able to train a decent classifier. Lastly, I divide the hand-coded data in a training and a test set and assign groups to the training data for cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(basedir + 'full_coded.csv', mode=\"r\", encoding=\"utf-8\") as fi:\n",
    "    with open(basedir + 'train.csv', mode=\"w\", encoding=\"utf-8\") as train:\n",
    "        with open(basedir + 'test.csv', mode=\"w\", encoding=\"utf-8\") as test:\n",
    "            with open(basedir + 'predict.csv', mode=\"w\", encoding=\"utf-8\") as predict:\n",
    "                reader = csv.reader(fi)\n",
    "                writer_train   = csv.writer(train, lineterminator='\\n')\n",
    "                writer_test    = csv.writer(test, lineterminator='\\n')\n",
    "                writer_predict = csv.writer(predict, lineterminator='\\n')\n",
    "                traintest = []\n",
    "                for row in reader:\n",
    "                    if row[7] == '':\n",
    "                        writer_predict.writerow(row)\n",
    "                    else:\n",
    "                        traintest.append(row)\n",
    "                        \n",
    "                # random sample 60:40 train:test\n",
    "                rsample = random.sample(range(len(traintest)), round(len(traintest)*.4))\n",
    "                i = 0\n",
    "                for row in traintest:\n",
    "                    if i in rsample:\n",
    "                        writer_test.writerow(row)\n",
    "                    else:\n",
    "                        writer_train.writerow(row)\n",
    "                    i += 1\n",
    "\n",
    "\n",
    "# define split-samples in training data for cross-vaidation\n",
    "with open(basedir + 'Releases/train.csv', mode=\"r\", encoding=\"utf-8\") as train:\n",
    "    with open(basedir + 'crossval.csv', mode=\"w\", encoding=\"utf-8\") as crossval:\n",
    "        reader = csv.reader(train)\n",
    "        fieldnames = ['date', 'sender', 'title', 'link', 'raw', 'clean_full', 'clean_rest', 'coding', 'val_sample']\n",
    "        writer = csv.DictWriter(crossval, lineterminator='\\n', fieldnames = fieldnames)\n",
    "        writer.writeheader()\n",
    "        next(reader) # skip header\n",
    "        for row in reader:\n",
    "            rsample = random.sample(range(5), 1)\n",
    "            row.append(rsample[0])\n",
    "            writer.writerow({'date':         row[0], \n",
    "                            'sender':       row[1], \n",
    "                            'title':        row[2], \n",
    "                            'link':         row[3],\n",
    "                            'raw':          row[4],\n",
    "                            'clean_full':   row[5],\n",
    "                            'clean_rest':   row[6], \n",
    "                            'coding':       row[7], \n",
    "                            'val_sample':   row[8]})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
